{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "创建数据\n",
    "首先, 我们这次需要加载 tensorflow 和 numpy 两个模块, 并且使用 numpy 来创建我们的数据."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data*0.1 + 0.3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接着, 我们用 tf.Variable 来创建描述 y 的参数. \n",
    "我们可以把 y_data = x_data*0.1 + 0.3 想象成 y=Weights * x + biases, \n",
    "然后神经网络也就是学着把 Weights 变成 0.1, biases 变成 0.3."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "biases = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "y = Weights*x_data + biases"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "计算误差\n",
    "接着就是计算 y 和 y_data 的误差:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y-y_data))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "传播误差\n",
    "反向传递误差的工作就教给optimizer了, \n",
    "我们使用的误差传递方法是梯度下降法: Gradient Descent 让后我们使用 optimizer 来进行参数的更新."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "训练\n",
    "到目前为止, 我们只是建立了神经网络的结构, 还没有使用这个结构. \n",
    "在使用这个结构之前, 我们必须先初始化所有之前定义的Variable, 所以这一步是很重要的!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法\n",
    "init = tf.global_variables_initializer()  # 替换成这样就好"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "接着,我们再创建会话 Session. 我们会在下一节中详细讲解 Session. \n",
    "我们用 Session 来执行 init 初始化步骤. 并且, 用 Session 来 run 每一次 training 的数据. \n",
    "逐步提升神经网络的预测准确性."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.3579099] [0.21562894]\n",
      "20 [0.15638004] [0.26620173]\n",
      "40 [0.11400712] [0.29160315]\n",
      "60 [0.10347996] [0.29791388]\n",
      "80 [0.10086457] [0.29948172]\n",
      "100 [0.10021479] [0.29987127]\n",
      "120 [0.10005337] [0.29996803]\n",
      "140 [0.10001326] [0.29999205]\n",
      "160 [0.1000033] [0.29999804]\n",
      "180 [0.10000084] [0.2999995]\n",
      "200 [0.10000023] [0.2999999]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)          # Very important\n",
    "\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(Weights), sess.run(biases))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
